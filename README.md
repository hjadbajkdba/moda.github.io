# MoDA: Multi-modal Diffusion Architecture for Talking Head Generation

## Overview

MoDA (Multi-modal Diffusion Architecture) addresses the challenges of talking head generation with arbitrary identities and speech audio in virtual environments. Current methods struggle with synthesizing diverse facial expressions and natural head movements while ensuring synchronized lip movements with the audio. MoDA introduces innovative technologies to overcome these issues, resulting in enhanced video diversity, realism, and efficiency.
## Anonymous demo website: https://hjadbajkdba.github.io/moda.github.io/

## Video Showcase

#### 1. Introduction Video**
[MoDA Demo Video](moda/moda.mp4)

#### 2. MoDA vs. Other Systems (moda, echomimic, hallo2, hallo, joyhallo, joyvasa, ditto)
{moda/compara,moda/compara2,moda/compara3,moda/compara4,moda/compara5} are videos showcasing different systems and comparisons.
#### 3. Talking Head Generation in Complex Scenarios
{moda/Complex Scenarios/} are Talking Head Generation in Complex Scenarios.
#### 4. Fine-grained Emotion Control
{moda/Emotion Contro} are Fine-grained Emotion Control.
#### 5. Long Videos Generation
{moda/Long Videos Generation} are Long Videos Generation.
#### 6. Ablation Study
{moda/Ablation Study} are Ablation Study


